{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44548,"status":"ok","timestamp":1650624179813,"user":{"displayName":"TECOB257_Chaitanya Palghadmal","userId":"17924064590604163241"},"user_tz":-330},"id":"fBj_8BnC69jD","outputId":"c9f58447-fe5a-4db2-8dfb-8dcf0717fb8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]}],"source":["#ROHIT MORE TECOB237\n","import nltk\n","nltk.download('all')\n","txt = \"Sukanya, Rajib and Naba are my good friends. \" \\\n","    \"Sukanya is getting married next year. \" \\\n","    \"Marriage is a big step in one’s life.\" \\\n","    \"It is both exciting and frightening. \" \\\n","    \"But friendship is a sacred bond between people.\" \\\n","    \"It is a special kind of love between us. \" \\\n","    \"Many of you must have tried searching for a friend \"\\\n","    \"but never found the right one.\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1650624199299,"user":{"displayName":"TECOB257_Chaitanya Palghadmal","userId":"17924064590604163241"},"user_tz":-330},"id":"-whMTqKr7N3Z","outputId":"ea9c73ef-170b-49fc-bfbb-fd4fb3258e51"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Sukanya', ',', 'Rajib', 'and', 'Naba', 'are', 'my', 'good', 'friends', '.', 'Sukanya', 'is', 'getting', 'married', 'next', 'year', '.', 'Marriage', 'is', 'a', 'big', 'step', 'in', 'one', '’', 's', 'life.It', 'is', 'both', 'exciting', 'and', 'frightening', '.', 'But', 'friendship', 'is', 'a', 'sacred', 'bond', 'between', 'people.It', 'is', 'a', 'special', 'kind', 'of', 'love', 'between', 'us', '.', 'Many', 'of', 'you', 'must', 'have', 'tried', 'searching', 'for', 'a', 'friend', 'but', 'never', 'found', 'the', 'right', 'one', '.']\n"]}],"source":["#Tokenization\n","from nltk.tokenize import word_tokenize\n","token = word_tokenize(txt)\n","print(token)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":449,"status":"ok","timestamp":1650624235740,"user":{"displayName":"TECOB257_Chaitanya Palghadmal","userId":"17924064590604163241"},"user_tz":-330},"id":"GWYSRPBr87Na","outputId":"c5880925-52c0-4b53-84f8-dbf930252cf2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sukanya ---> sukanya\n",", ---> ,\n","Rajib ---> rajib\n","and ---> and\n","Naba ---> naba\n","are ---> are\n","my ---> my\n","good ---> good\n","friends ---> friend\n",". ---> .\n","Sukanya ---> sukanya\n","is ---> is\n","getting ---> get\n","married ---> marri\n","next ---> next\n","year ---> year\n",". ---> .\n","Marriage ---> marriag\n","is ---> is\n","a ---> a\n","big ---> big\n","step ---> step\n","in ---> in\n","one ---> one\n","’ ---> ’\n","s ---> s\n","life.It ---> life.it\n","is ---> is\n","both ---> both\n","exciting ---> excit\n","and ---> and\n","frightening ---> frighten\n",". ---> .\n","But ---> but\n","friendship ---> friendship\n","is ---> is\n","a ---> a\n","sacred ---> sacr\n","bond ---> bond\n","between ---> between\n","people.It ---> people.it\n","is ---> is\n","a ---> a\n","special ---> special\n","kind ---> kind\n","of ---> of\n","love ---> love\n","between ---> between\n","us ---> us\n",". ---> .\n","Many ---> mani\n","of ---> of\n","you ---> you\n","must ---> must\n","have ---> have\n","tried ---> tri\n","searching ---> search\n","for ---> for\n","a ---> a\n","friend ---> friend\n","but ---> but\n","never ---> never\n","found ---> found\n","the ---> the\n","right ---> right\n","one ---> one\n",". ---> .\n"]}],"source":["#Stemming\n","from nltk.stem import PorterStemmer\n","ps = PorterStemmer()\n","for w in token:\n","  print(w, \"--->\", ps.stem(w))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1852,"status":"ok","timestamp":1650624258961,"user":{"displayName":"TECOB257_Chaitanya Palghadmal","userId":"17924064590604163241"},"user_tz":-330},"id":"k4JDLYlj98h5","outputId":"421bd941-e6e3-41d4-ffcd-8250ed7e35d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sukanya\n",",\n","Rajib\n","and\n","Naba\n","are\n","my\n","good\n","friend\n",".\n","Sukanya\n","is\n","getting\n","married\n","next\n","year\n",".\n","Marriage\n","is\n","a\n","big\n","step\n","in\n","one\n","’\n","s\n","life.It\n","is\n","both\n","exciting\n","and\n","frightening\n",".\n","But\n","friendship\n","is\n","a\n","sacred\n","bond\n","between\n","people.It\n","is\n","a\n","special\n","kind\n","of\n","love\n","between\n","u\n",".\n","Many\n","of\n","you\n","must\n","have\n","tried\n","searching\n","for\n","a\n","friend\n","but\n","never\n","found\n","the\n","right\n","one\n",".\n"]}],"source":["#Lemmatization\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","for w in token:\n","  print(lemmatizer.lemmatize(w))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":698,"status":"ok","timestamp":1650624291026,"user":{"displayName":"TECOB257_Chaitanya Palghadmal","userId":"17924064590604163241"},"user_tz":-330},"id":"AEYjQOtI-bri","outputId":"bfd9ffa4-5213-4883-bb33-2682c7d8fd04"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Sukanya', ',', 'Rajib', 'Naba', 'good', 'friends', '.', 'Sukanya', 'getting', 'married', 'next', 'year', '.', 'Marriage', 'big', 'step', 'one', '’', 'life.It', 'exciting', 'frightening', '.', 'But', 'friendship', 'sacred', 'bond', 'people.It', 'special', 'kind', 'love', 'us', '.', 'Many', 'must', 'tried', 'searching', 'friend', 'never', 'found', 'right', 'one', '.']\n"]}],"source":["#Stop Words\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","filtered_sentence = [w for w in token if not w.lower() in stop_words]\n"," \n","filtered_sentence = []\n","for w in token:\n","    if w not in stop_words:\n","        filtered_sentence.append(w)\n","print(filtered_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":564,"status":"ok","timestamp":1650624598567,"user":{"displayName":"TECOB257_Chaitanya Palghadmal","userId":"17924064590604163241"},"user_tz":-330},"id":"L3KAWCq74Azj","outputId":"f982620e-b861-46f2-cf90-58abba53fd09"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('Sukanya', 'NN')]\n","[(',', ',')]\n","[('Rajib', 'NN')]\n","[]\n","[('Naba', 'NN')]\n","[]\n","[]\n","[('good', 'JJ')]\n","[('friends', 'NNS')]\n","[('.', '.')]\n","[('Sukanya', 'NN')]\n","[]\n","[('getting', 'VBG')]\n","[('married', 'JJ')]\n","[('next', 'JJ')]\n","[('year', 'NN')]\n","[('.', '.')]\n","[('Marriage', 'NN')]\n","[]\n","[]\n","[('big', 'JJ')]\n","[('step', 'NN')]\n","[]\n","[('one', 'CD')]\n","[('’', 'NN')]\n","[]\n","[('life.It', 'NN')]\n","[]\n","[]\n","[('exciting', 'VBG')]\n","[]\n","[('frightening', 'VBG')]\n","[('.', '.')]\n","[('But', 'CC')]\n","[('friendship', 'NN')]\n","[]\n","[]\n","[('sacred', 'VBN')]\n","[('bond', 'NN')]\n","[]\n","[('people.It', 'NN')]\n","[]\n","[]\n","[('special', 'JJ')]\n","[('kind', 'NN')]\n","[]\n","[('love', 'NN')]\n","[]\n","[('us', 'PRP')]\n","[('.', '.')]\n","[('Many', 'JJ')]\n","[]\n","[]\n","[('must', 'MD')]\n","[]\n","[('tried', 'VBN')]\n","[('searching', 'VBG')]\n","[]\n","[]\n","[('friend', 'NN')]\n","[]\n","[('never', 'RB')]\n","[('found', 'NN')]\n","[]\n","[('right', 'NN')]\n","[('one', 'CD')]\n","[('.', '.')]\n"]}],"source":["#POS Tagging\n","for i in token:\n","  wordlist = nltk.word_tokenize(i)\n","  wordlist = [w for w in wordlist if not w in stop_words]\n","  tagged = nltk.pos_tag(wordlist)\n","  print(tagged)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOWD+gRSh40WFAt3YEYmEpD","collapsed_sections":[],"name":"DSBDA Assignment 7.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
